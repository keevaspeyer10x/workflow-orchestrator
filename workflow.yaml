name: "General Development Workflow"
version: "2.1"
description: "A general-purpose 5-phase workflow for software development tasks. Works with any project type (Node.js, Python, etc.). Default to using Claude Code for implementation unless task is trivial."

# =============================================================================
# SETUP REQUIREMENTS - READ THIS FIRST!
# =============================================================================
# Before starting this workflow, ensure:
# 1. Claude Code CLI is installed: npm install -g @anthropic-ai/claude-code
# 2. Required environment variables are set (check with: env | grep -i api_key)
# 3. Project dependencies are installed
#
# CRITICAL: API KEYS FOR EXTERNAL REVIEWS
# External AI reviews are MANDATORY for code changes. Load secrets with:
#   eval "$(sops -d secrets.enc.yaml | sed 's/: /=/' | sed 's/^/export /')"
#
# Required keys:
# - GEMINI_API_KEY: For Gemini 3 Pro reviews (architecture, design)
# - OPENAI_API_KEY: For GPT-5.2 Max / Codex reviews (security, correctness)
# - OPENROUTER_API_KEY: For API fallback when CLIs unavailable
# - XAI_API_KEY: For Grok 4.1 reviews (operations, edge cases)
#
# If keys are not loaded:
# - REVIEW phase items will FAIL
# - Workflow completion will show "No external model reviews recorded!"
# - You will need to reload keys and re-run reviews
# =============================================================================

settings:
  # Configurable settings - override these per-project
  # Test command - defaults to build check
  # Override with actual test command: "npm test", "pytest", "go test ./...", etc.
  test_command: "python -m pytest tests/ -v --tb=short"
  
  # Build/compile command (optional, for compiled languages)
  build_command: "npm run build"
  
  # Lint command (optional)
  lint_command: "npm run lint"
  
  # Documentation directory
  docs_dir: "docs"
  
  # Tests directory
  tests_dir: "tests"

  # Smoke test command - lightweight runtime verification
  # Examples:
  #   Web: "playwright test tests/smoke/"
  #   CLI: "orchestrator --version && orchestrator status"
  #   API: "curl -f http://localhost:8000/health"
  smoke_test_command: "python -m pytest tests/smoke/ -v --tb=short"

  # Supervision mode - how much human oversight is required
  # - zero_human: Fully autonomous, skip manual gates with warning (AI-only code review)
  # - supervised: Require human approval at manual gates (traditional workflow)
  # - hybrid: Risk-based gates with timeout fallback (auto-approve low-risk)
  # NOTE: Production deployments always require human approval regardless of mode
  supervision_mode: "zero_human"

  # Claude Code is the default executor - only skip for trivial tasks
  default_executor: "claude_code"
  
  # Model selection for multi-agent reviews
  # Principle: Use latest generation model available, don't hardcode specific versions
  # Examples: "gpt-4.1-mini" -> use latest GPT, "claude-3" -> use latest Claude
  review_model: "latest_available"

  # Multi-model review configuration
  # Reviews are ON by default to catch issues in AI-generated code
  reviews:
    enabled: true
    on_by_default: true  # Reviews run automatically in REVIEW phase

    # Execution method: auto | cli | api | github-actions
    # - cli: Codex CLI + Gemini CLI (full repo access, best experience)
    # - api: OpenRouter API (context injection, works in Claude Code Web)
    # - github-actions: Triggered on PR (requires setup-reviews)
    method: auto

    # Review types and their models
    # Security + Quality use Codex (code-specialized)
    # Consistency + Holistic use Gemini (1M context for codebase understanding)
    types:
      security_review: codex
      quality_review: codex
      consistency_review: gemini
      holistic_review: gemini

    # Model versions - UPDATE THESE when new models are released
    # CLI models are used when running Codex/Gemini CLIs directly
    # API models are used when routing through OpenRouter
    # NOTE: Use model_registry.get_latest_model() to get current versions dynamically
    models:
      cli:
        codex: "gpt-5.1-codex-max"       # Latest Codex model
        gemini: "gemini-3-pro-preview"   # Latest Gemini model (preview)
        grok: "grok-4.1-fast"  # Latest Grok model (Jan 2026)
      api:
        codex: "openai/gpt-5.1"          # OpenRouter model ID
        gemini: "google/gemini-3-pro-preview"  # OpenRouter model ID
        grok: "x-ai/grok-4.1-fast"   # OpenRouter model ID

    # GitHub Actions configuration
    github_actions:
      required_for_merge: true  # Block PR merge if reviews fail

    # Fallback when preferred method unavailable
    fallback_to_api: true

    # Review reliability - graceful degradation for zero-human workflows
    # Require minimum N of 5 reviews to succeed (prevents single API key blocking workflow)
    minimum_required: 3  # At least 3 of 5 reviews must complete successfully

    # Fallback models when primary APIs unavailable (missing keys, rate limits, outages)
    # Uses OpenRouter as universal fallback, then Claude Opus as final fallback
    fallbacks:
      codex:
        - "openai/gpt-5.1"           # OpenRouter fallback
        - "anthropic/claude-opus-4"  # Final fallback
      gemini:
        - "google/gemini-3-pro"      # OpenRouter fallback
        - "anthropic/claude-opus-4"  # Final fallback
      grok:
        - "x-ai/grok-4.1"            # OpenRouter fallback
        - "anthropic/claude-opus-4"  # Final fallback

phases:
  # ===========================================================================
  # PHASE 0: SETUP (implicit, run before workflow starts)
  # ===========================================================================
  # The orchestrator should verify these before starting:
  # - [ ] Claude Code CLI installed and accessible
  # - [ ] Required API keys present in environment
  # - [ ] Project can be built (test_command runs successfully)
  # - [ ] Git repository is clean or changes are stashed

  # ===========================================================================
  # PHASE 1: PLAN
  # ===========================================================================
  - id: "PLAN"
    name: "Planning & Scoping"
    description: "Define the work to be done, assess risks, and get approval before starting. ASK CLARIFYING QUESTIONS before proceeding."
    items:
      - id: "check_roadmap"
        name: "Review Roadmap/Backlog"
        description: "Check if any existing roadmap items or backlog issues should be addressed alongside this task. Review ROADMAP.md and/or GitHub issues if used. Ask user if any should be included in scope."
        required: false
        skippable: true
        skip_conditions: ["new_project", "no_roadmap_exists"]

      - id: "clarifying_questions"
        name: "Ask Clarifying Questions"
        description: "Before creating a plan, ask the user clarifying questions to ensure full understanding. For EACH question you MUST provide: 1) Your RECOMMENDATION (what you think is best), 2) ALTERNATIVES (other valid options), 3) TRADEOFFS (why you recommend one over others). Format: '**Q1: [Question]** | Recommendation: X | Alternatives: Y, Z | Tradeoffs: ...' This helps users make informed decisions quickly."
        required: true
        skippable: true
        skip_conditions: ["requirements_crystal_clear", "user_provided_full_context"]
        notes:
          - "[caution] Never ask open-ended questions without recommendations. Users should be able to say 'yes, use your recommendations' and move forward."
          - "[tip] Format each question as: Question â†’ Recommendation â†’ Alternatives â†’ Tradeoffs"
          - "[example] 'Should we include run_command? Recommendation: No (security risk). Alternatives: Yes with allowlist, Yes unrestricted. Tradeoffs: Security vs flexibility.'"

      - id: "initial_plan"
        name: "Generate initial plan"
        description: "Create a detailed plan of action and save it to {{docs_dir}}/plan.md. Include decision on whether to use Claude Code (default: yes) or handle directly (only for trivial tasks)."
        required: true
        skippable: false
        verification:
          type: "file_exists"
          path: "{{docs_dir}}/plan.md"

      - id: "risk_analysis"
        name: "Risk & Impact Analysis"
        description: "Document potential risks and their impact in {{docs_dir}}/risk_analysis.md"
        required: true
        skippable: true
        skip_conditions: ["simple_bug_fix", "trivial_change"]
        verification:
          type: "file_exists"
          path: "{{docs_dir}}/risk_analysis.md"

      - id: "define_test_cases"
        name: "Define Test Cases"
        description: "Outline the test cases that will be used to verify the feature in {{tests_dir}}/test_cases.md. For UI changes, include visual/screenshot test cases."
        required: true
        skippable: false
        verification:
          type: "file_exists"
          path: "{{tests_dir}}/test_cases.md"

      - id: "parallel_execution_check"
        name: "Assess Parallel Execution Opportunity"
        description: "Before starting implementation, determine if tasks can be parallelized using multiple agents. Document your decision and reasoning."
        required: true
        skippable: true
        skip_conditions: ["single_task_only", "tasks_are_dependent"]
        notes:
          - "[critical] Are there 2+ independent tasks? Consider parallel agents (e.g., 'orchestrator prd spawn --count 3' or multiple Task tool calls)"
          - "[howto] Launch parallel agents: Send ONE message with MULTIPLE Task tool calls - NOT separate messages"
          - "[example] Good: Task(description='Fix auth', ...) + Task(description='Fix API', ...) in SAME message"
          - "[example] Bad: Task for auth in message 1, then Task for API in message 2 (sequential, not parallel)"
          - "[plan] Use Plan agent FIRST (subagent_type='Plan') if implementation approach is unclear"
          - "[verify] Will you verify agent output by reading files, not trusting summaries?"
          - "[decision] Document in plan.md: Will use [sequential/parallel] execution because [reason]"
          - "[learning] From WF-034: Multiple sessions showed agents launching 1 agent instead of 3-4 parallel agents, wasting time"

      - id: "user_approval"
        name: "Get User Approval"
        description: "The user must approve the plan before execution can begin."
        required: true
        skippable: false
        verification:
          type: "manual_gate"
          description: "User must run 'orchestrator approve-item user_approval' to proceed."

  # ===========================================================================
  # PHASE 2: EXECUTE
  # ===========================================================================
  - id: "EXECUTE"
    name: "Implementation"
    description: "Write code and tests to implement the planned feature. DEFAULT: Use Claude Code for implementation. Only handle directly for trivial tasks with explicit justification."
    notes:
      - "[caution] Before implementing, verify that your chosen agent/provider is actually available. If Claude Code CLI is not installed, check for alternative connectors (e.g., Manus direct connection, OpenRouter API)."
      - "[tip] If the preferred agent is unavailable, ASK THE USER before defaulting to manual implementation. They may know of alternative connection methods."
      - "[learning] From v2.2: Claude Code CLI may not be available in all environments (e.g., Manus sandbox), but alternative connectors often exist."
    items:
      - id: "handoff_decision"
        name: "Claude Code Handoff Decision"
        description: "Determine if task should be handed off to Claude Code (default) or handled directly. Document reasoning. Only skip Claude Code for: 1) Single-line config changes, 2) Documentation-only updates, 3) Explicit user request to handle directly."
        required: true
        skippable: false
        notes:
          - "[caution] BEFORE deciding, verify agent availability: 1) Check if Claude Code CLI is installed (`which claude`), 2) Check for Manus direct connector, 3) Check for OpenRouter API key. If primary agent unavailable, ASK USER about alternatives before proceeding."
          - "[tip] Use `orchestrator handoff --provider auto` to auto-detect the best available provider for your environment."
          - "[learning] Different environments have different agent access: Claude Code CLI (local dev), Manus direct connector (Manus sandbox), OpenRouter API (universal fallback)."

      - id: "write_tests"
        name: "Write tests (RED phase)"
        description: "Write test code from test_cases.md BEFORE implementation. Tests should fail initially - this verifies they actually test something. TDD: Red-Green-Refactor."
        required: true
        skippable: false
        notes:
          - "[caution] Write tests BEFORE implementation code. Tests that pass before code is written are not testing anything."
          - "[tip] Run tests after writing - they SHOULD fail. If they pass, the test is not testing new functionality."
          - "[learning] Post-implementation tests tend to verify 'what was built' rather than 'what should be built', leading to gaps."

      - id: "implement_code"
        name: "Implement feature code (GREEN phase)"
        description: "Write the minimal application code to make tests pass. TDD: Red-Green-Refactor."
        required: true
        skippable: false
        notes:
          - "[tip] Write just enough code to make the failing tests pass. Refactor later."

      - id: "all_tests_pass"
        name: "Ensure all tests pass"
        description: "Run the test suite and ensure all tests pass after implementation."
        required: true
        skippable: false
        verification:
          type: "command"
          command: "{{test_command}}"
          expect_exit_code: 0

  # ===========================================================================
  # PHASE 3: REVIEW
  # ===========================================================================
  # IMPORTANT: All 4 reviews use EXTERNAL MODELS (not the implementation model)
  # Reviews should be started IN THE BACKGROUND at phase start to run in parallel
  # API keys required: GEMINI_API_KEY, OPENAI_API_KEY, OPENROUTER_API_KEY
  # Load keys with: eval "$(sops -d secrets.enc.yaml | sed 's/: /=/' | sed 's/^/export /')"
  # ===========================================================================
  - id: "REVIEW"
    name: "Multi-Model Code Review"
    description: "Run ALL 4 external model reviews. Security+Quality use Codex, Consistency+Holistic use Gemini. Start reviews in BACKGROUND to run in parallel. Do NOT skip any review without explicit user approval."
    notes:
      - "[critical] ALL 4 REVIEWS ARE MANDATORY. Do not skip without user approval."
      - "[critical] START ALL REVIEWS IN BACKGROUND at phase start to run in parallel."
      - "[critical] API keys required: GEMINI_API_KEY, OPENAI_API_KEY. Load with: eval $(sops -d secrets.enc.yaml)"
      - "[tip] Run all reviews in parallel in background for speed."
      - "[learning] Gemini (1M context) catches codebase-wide issues that Codex misses."
    items:
      - id: "security_review"
        name: "Security Review [Codex]"
        description: "MANDATORY. Uses Codex/GPT-5.1 to check for OWASP Top 10, injection attacks, auth issues, SSRF, hardcoded secrets. Run in BACKGROUND."
        step_type: "gate"
        skip_conditions: ["documentation_only"]
        notes:
          - "[model] Codex (gpt-5.1-codex-max) - code-specialized security analysis"
          - "[background] Start this review in background, don't wait for completion"

      - id: "quality_review"
        name: "Code Quality Review [Codex]"
        description: "MANDATORY. Uses Codex/GPT-5.1 to check for code smells, edge cases, error handling, test coverage gaps. Run in BACKGROUND."
        step_type: "gate"
        skip_conditions: ["documentation_only"]
        notes:
          - "[model] Codex (gpt-5.1-codex-max) - code-specialized quality analysis"
          - "[background] Start this review in background, don't wait for completion"

      - id: "consistency_review"
        name: "Codebase Consistency Review [Gemini]"
        description: "MANDATORY. Uses Gemini 3 Pro (1M context) to check if new code fits existing patterns, uses existing utilities, follows conventions. Run in BACKGROUND."
        step_type: "gate"
        skip_conditions: ["documentation_only"]
        notes:
          - "[model] Gemini 3 Pro (gemini-3-pro-preview) - 1M context for full codebase"
          - "[background] Start this review in background, don't wait for completion"
          - "[key] Requires GEMINI_API_KEY environment variable"

      - id: "holistic_review"
        name: "Holistic Senior Engineer Review [Gemini]"
        description: "MANDATORY. Uses Gemini 3 Pro as a 'skeptical senior engineer' - would you approve this PR? What concerns you? What questions would you ask? Run in BACKGROUND."
        step_type: "gate"
        skip_conditions: ["documentation_only"]
        notes:
          - "[model] Gemini 3 Pro (gemini-3-pro-preview) - fresh eyes review"
          - "[background] Start this review in background, don't wait for completion"
          - "[key] Requires GEMINI_API_KEY environment variable"

      - id: "vibe_coding_review"
        name: "Vibe-Coding Issues Review [Grok]"
        description: "MANDATORY. Uses Grok 3 to catch AI-specific issues: hallucinated APIs, plausible-but-wrong logic, tests that don't test, cargo cult code. Third model perspective. Run in BACKGROUND."
        step_type: "gate"
        skip_conditions: ["documentation_only"]
        notes:
          - "[model] Grok 3 - catches AI-generation blindspots"
          - "[background] Start this review in background, don't wait for completion"
          - "[key] Requires XAI_API_KEY environment variable"
          - "[vibe] Specifically designed for zero-human-review workflows"

      - id: "collect_review_results"
        name: "Collect All Review Results"
        description: "Wait for all 5 background reviews to complete. Aggregate findings. Report which models were used and any issues found. Block if any CRITICAL issues."
        step_type: "gate"
        notes:
          - "[gate] Do not proceed until ALL 5 reviews have completed"
          - "[report] List each review with: model used, duration, issues found"
          - "[block] If any review found CRITICAL issues, do not advance phase"
          - "[models] Codex (security, quality), Gemini (consistency, holistic), Grok (vibe_coding)"

  # ===========================================================================
  # PHASE 4: VERIFY
  # ===========================================================================
  - id: "VERIFY"
    name: "Final Verification"
    description: "Perform final checks to ensure the feature is ready for release."
    items:
      - id: "full_test_suite"
        name: "Run full test suite"
        description: "Ensure no regressions were introduced during review."
        required: true
        skippable: false
        verification:
          type: "command"
          command: "{{test_command}}"
          expect_exit_code: 0

      - id: "visual_regression_test"
        name: "Visual Regression Test (Playwright)"
        description: "For UI changes: automated visual regression testing using Playwright screenshots. Compares against baseline to catch unintended visual changes."
        required: true
        skippable: true
        skip_conditions: ["no_ui_changes", "backend_only", "api_only"]
        verification:
          type: "command"
          command: "playwright test --grep @visual"
          expect_exit_code: 0
        notes:
          - "[tool] Uses Playwright for screenshot capture and pixel-diff comparison"
          - "[baseline] First run: playwright test --update-snapshots (creates baseline)"
          - "[compare] Subsequent runs compare to baseline in tests/**/__snapshots__/"
          - "[threshold] Configure pixel diff tolerance in playwright.config.ts (e.g., maxDiffPixels: 100)"
          - "[ci] In CI/CD mode, never auto-update snapshots (fail on any mismatch)"
          - "[setup] Install: npm install -D @playwright/test && npx playwright install"
          - "[guide] https://playwright.dev/docs/test-snapshots"
          - "[zero-human] Fully automated, no manual comparison needed"

      - id: "automated_smoke_test"
        name: "Automated Smoke Test"
        description: "Run automated smoke test suite to verify core functionality. Replaces manual smoke test in zero-human mode."
        required: true
        skippable: true
        skip_conditions: ["no_smoke_tests_defined"]
        verification:
          type: "command"
          command: "{{smoke_test_command}}"
          expect_exit_code: 0
        notes:
          - "[zero-human] Automated runtime verification (replaces manual testing)"
          - "[web] Example: playwright test tests/smoke/ (test login, navigation, core flows)"
          - "[cli] Example: orchestrator --version && orchestrator status (verify CLI works)"
          - "[api] Example: curl -f http://localhost:8000/health (verify endpoints respond)"
          - "[scope] Smoke tests are lightweight - test critical paths only (not exhaustive)"
          - "[speed] Should complete in < 2 minutes (fast feedback loop)"

  # ===========================================================================
  # PHASE 5: LEARN
  # ===========================================================================
  - id: "LEARN"
    name: "Learning & Documentation"
    description: "Document learnings, propose actions, get user approval, then apply. Actions are only embedded after explicit user approval."
    notes:
      - "[caution] Never apply learnings to workflow/roadmap without user approval first."
      - "[tip] Propose specific actions, not vague improvements. User should know exactly what will change."
    items:
      - id: "root_cause_analysis"
        name: "Root Cause Analysis"
        description: "MANDATORY: Perform and document root cause analysis. Why did this issue occur? What was the underlying cause, not just the symptom? Document in LEARNINGS.md."
        required: true
        skippable: false

      - id: "document_learnings"
        name: "Document Learnings"
        description: "Create or update LEARNINGS.md with: 1) Problem summary, 2) Root cause analysis, 3) Fix applied, 4) Systemic insights, 5) Prevention measures."
        required: true
        skippable: false
        verification:
          type: "file_exists"
          path: "LEARNINGS.md"

      - id: "propose_actions"
        name: "Propose Recommended Actions"
        description: "Based on learnings, propose specific actions to prevent recurrence. List each action clearly: 1) What to change, 2) Which file(s) affected, 3) Whether it's immediate (apply now) or roadmap (future). Present to user for approval."
        required: true
        skippable: false
        notes:
          - "[critical] For each ROADMAP item, include mandatory complexity vs benefit tradeoff analysis"
          - "[critical] Categorize each item as: âœ… RECOMMEND (implement) / âš ï¸ DEFER (no evidence of need) / ðŸ” EXPLORATORY (needs separate analysis)"
          - "[template] Use standard tradeoff template: Complexity table + Current Evidence + YAGNI Check + Recommendation with reasoning"
          - "[yagni] Is this solving a problem we actually have? Would we be okay WITHOUT this for 6-12 months? Does current solution fail in practice?"
          - "[evidence] Include: âœ…/âŒ Production data showing need, âœ…/âŒ User requests, âœ…/âŒ Observed bottleneck, âœ…/âŒ Compliance requirement"
          - "[tip] Be specific: 'Reorder EXECUTE items in workflow.yaml' not 'improve workflow'"
          - "[tip] Categorize: APPLY NOW (small, safe) vs ROADMAP (large, needs design)"
          - "[caution] User must approve actions before any changes are made"
          - "[learning] From PRD-007: 4 out of 5 proposed items had no evidence of need - tradeoff analysis prevents roadmap bloat"

      - id: "approve_actions"
        name: "User Approval of Proposed Actions"
        description: "User reviews proposed actions and approves which ones to apply. User may reject, modify, or defer actions. Only approved actions proceed to implementation."
        required: true
        skippable: false
        verification:
          type: "manual_gate"
          description: "User must review proposed actions and run 'orchestrator approve-item approve_actions' to confirm which actions to apply."

      - id: "apply_approved_actions"
        name: "Apply Approved Actions"
        description: "Apply the user-approved actions: update workflow.yaml, add to ROADMAP.md, backport to bundled defaults, etc. Only apply what was explicitly approved."
        required: true
        skippable: true
        skip_conditions: ["no_actions_approved"]
        notes:
          - "[caution] Only apply actions the user approved in previous step"
          - "[tip] For workflow changes, update both workflow.yaml AND src/default_workflow.yaml"

      - id: "update_knowledge_base"
        name: "Update Knowledge Base"
        description: "Update any relevant project documentation with approved learnings."
        required: false
        skippable: true

      - id: "update_documentation"
        name: "Update User-Facing Documentation"
        description: "Review and update user-facing documentation based on changes made. Consider: CHANGELOG.md, README.md, API docs."
        required: true
        skippable: true
        skip_conditions: ["internal_refactor_only", "no_user_facing_changes"]
        notes:
          - "[changelog] Update CHANGELOG.md using Keep a Changelog format (Added/Changed/Fixed/Removed/Security)"
          - "[readme] Update README.md if: new features, setup changes, usage changes, new dependencies"
          - "[api] Update API documentation if public interfaces changed"
          - "[tip] The commit message often contains good starting content for changelog entries"
          - "[caution] Don't skip this for user-facing changes - outdated docs frustrate users"

      - id: "capture_workflow_feedback"
        name: "Capture Workflow Feedback (Automatic)"
        description: "AUTOMATIC: Capture structured feedback about workflow adherence and experience. This runs automatically and is opt-out (set ORCHESTRATOR_SKIP_FEEDBACK=1 to disable). Saves to .workflow_feedback.jsonl for aggregation across repos."
        required: true
        skippable: true
        skip_conditions: ["feedback_disabled_by_env_var"]
        notes:
          - "[auto] This runs AUTOMATICALLY using 'orchestrator feedback --auto' - no user prompt needed"
          - "[questions] Captures: parallel agents used? reviews performed? what went well? challenges? improvements?"
          - "[storage] Saves to .workflow_feedback.jsonl (gitignored, safe for aggregation)"
          - "[opt-out] To disable: export ORCHESTRATOR_SKIP_FEEDBACK=1"
          - "[learning] From WF-034: Two sessions showed identical problems - systematic feedback needed"
          - "[privacy] Only captures workflow metadata, no code or sensitive data"

      - id: "commit_and_sync"
        name: "Commit and Sync Changes"
        description: "Ask user if they are ready to commit and sync to main. If approved: 1) Run git status/diff, 2) Auto-generate commit message from workflow task and changes, 3) Stage relevant files (exclude .env, secrets, workflow state), 4) Commit and push."
        required: true
        skippable: true
        skip_conditions: ["no_changes_to_commit", "user_will_commit_manually"]
        verification:
          type: "manual_gate"
          description: "User confirms ready to commit and sync changes to main."
        notes:
          - "[tip] Auto-generate commit message: summarize the task from workflow state + key changes from git diff"
          - "[tip] Commit message format: 'feat/fix/chore: <summary from task>' with bullet points of what changed"
          - "[caution] Exclude from commit: .env, .workflow_state.json, .workflow_log.jsonl, secrets, credentials"
