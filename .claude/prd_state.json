{
  "prd_id": "prd-process-compliance",
  "prd_title": "Process Compliance Fixes (WF-012, WF-013, WF-014, WF-015)",
  "prd_file": "/home/keeva/workflow-orchestrator/process-compliance.prd.yaml",
  "tasks": [
    {
      "id": "wf-014-review-validation",
      "description": "## WF-014: Block Workflow Finish Without Required Reviews\n\n**Goal:** Prevent `orchestrator finish` from completing if required external\nmodel reviews were not run.\n\n**Implementation:**\n\n1. Add method to `src/engine.py`:\n   ```python\n   def get_completed_reviews(self) -> set[str]:\n       \"\"\"Get set of completed review types from workflow log.\"\"\"\n       completed = set()\n       for event in self.get_events():\n           if event.event_type == EventType.REVIEW_COMPLETED:\n               review_type = event.details.get(\"review_type\")\n               if review_type:\n                   completed.add(review_type)\n       return completed\n\n   def validate_reviews_completed(self) -> tuple[bool, list[str]]:\n       \"\"\"Check if required reviews were completed.\"\"\"\n       required = {\"security\", \"quality\"}  # Minimum required\n       completed = self.get_completed_reviews()\n       missing = required - completed\n       return len(missing) == 0, list(missing)\n   ```\n\n2. Add validation to `cmd_finish()` in `src/cli.py`:\n   - Check `engine.validate_reviews_completed()` before completing\n   - If missing reviews, print warning and block unless `--skip-review-check` flag\n   - Add `--skip-review-check` argument with required `--reason`\n\n3. Add tests to `tests/test_process_compliance.py`:\n   - Test get_completed_reviews returns review types\n   - Test validate_reviews_completed passes/fails correctly\n   - Test cmd_finish blocks without reviews\n\n**Files to modify:**\n- src/engine.py\n- src/cli.py\n- tests/test_process_compliance.py (tests already exist - make them pass)\n\n**Success criteria:**\n- `orchestrator finish` blocks if reviews missing\n- `orchestrator finish --skip-review-check --reason \"...\"` allows bypass\n- All TestReviewValidation tests pass\n",
      "dependencies": [],
      "status": "done"
    },
    {
      "id": "wf-012-context-reminder",
      "description": "## WF-012: Context Reminder Command\n\n**Goal:** Add a command that outputs compact workflow state suitable for\nre-injection after context compaction.\n\n**Implementation:**\n\n1. Add method to `src/engine.py`:\n   ```python\n   def get_context_reminder(self) -> dict:\n       \"\"\"Get compact workflow state for context injection.\"\"\"\n       if not self.state:\n           return {\"active\": False}\n\n       return {\n           \"active\": True,\n           \"task\": self.state.task_description,\n           \"phase\": self.state.current_phase_id,\n           \"progress\": f\"{self._count_completed()}/{self._count_total()}\",\n           \"constraints\": self.state.constraints or [],\n       }\n   ```\n\n2. Add CLI command `context-reminder` to `src/cli.py`:\n   ```python\n   def cmd_context_reminder(args):\n       engine = get_engine(args)\n       reminder = engine.get_context_reminder()\n       print(json.dumps(reminder))\n   ```\n\n3. Register the command in argument parser.\n\n4. Add tests - make TestContextReminder tests pass.\n\n**Files to modify:**\n- src/engine.py\n- src/cli.py\n- tests/test_process_compliance.py (tests already exist)\n\n**Success criteria:**\n- `orchestrator context-reminder` outputs JSON\n- Output includes active, task, phase, progress, constraints\n- All TestContextReminder tests pass\n",
      "dependencies": [],
      "status": "done"
    },
    {
      "id": "wf-013-verify-write",
      "description": "## WF-013: Verify Write Allowed Command\n\n**Goal:** Add a command that checks if implementation code should be written\nbased on current workflow phase.\n\n**Implementation:**\n\n1. Add method to `src/engine.py`:\n   ```python\n   def verify_write_allowed(self) -> tuple[bool, str]:\n       \"\"\"Check if writing implementation code is allowed.\"\"\"\n       if not self.state:\n           return True, \"No active workflow - write allowed\"\n\n       phase = self.state.current_phase_id\n       if phase == \"EXECUTE\":\n           return True, \"In EXECUTE phase - write allowed\"\n       else:\n           return False, f\"In {phase} phase - writing implementation code not allowed. Complete {phase} phase first.\"\n   ```\n\n2. Add CLI command `verify-write-allowed` to `src/cli.py`:\n   ```python\n   def cmd_verify_write_allowed(args):\n       engine = get_engine(args)\n       allowed, reason = engine.verify_write_allowed()\n       if allowed:\n           print(f\"\u2713 {reason}\")\n           sys.exit(0)\n       else:\n           print(f\"\u2717 {reason}\")\n           sys.exit(1)\n   ```\n\n3. Register the command in argument parser.\n\n4. Add tests - make TestVerifyWriteAllowed tests pass.\n\n**Files to modify:**\n- src/engine.py\n- src/cli.py\n- tests/test_process_compliance.py (tests already exist)\n\n**Success criteria:**\n- `orchestrator verify-write-allowed` returns 0 if allowed, 1 if not\n- Returns True when no workflow or in EXECUTE phase\n- Returns False in PLAN, REVIEW, VERIFY, LEARN phases\n- All TestVerifyWriteAllowed tests pass\n",
      "dependencies": [],
      "status": "done"
    },
    {
      "id": "wf-015-status-json",
      "description": "## WF-015: Status --json Flag\n\n**Goal:** Add `--json` flag to status command for machine-readable output.\n\n**Implementation:**\n\n1. Add method to `src/engine.py`:\n   ```python\n   def get_status_json(self) -> dict:\n       \"\"\"Get workflow status as JSON-serializable dict.\"\"\"\n       if not self.state:\n           return {\"active\": False}\n\n       return {\n           \"active\": True,\n           \"workflow_id\": self.state.workflow_id,\n           \"task\": self.state.task_description,\n           \"phase\": self.state.current_phase_id,\n           \"progress\": {\n               \"completed\": self._count_completed(),\n               \"total\": self._count_total(),\n           },\n           \"items\": self._get_current_phase_items(),\n           \"constraints\": self.state.constraints or [],\n       }\n   ```\n\n2. Add `--json` flag to status command in `src/cli.py`:\n   ```python\n   def cmd_status(args):\n       engine = get_engine(args)\n\n       if args.json:\n           status = engine.get_status_json()\n           print(json.dumps(status))\n           return\n\n       # ... existing status display ...\n   ```\n\n3. Add `--json` argument to status parser.\n\n4. Add tests - make TestStatusJson tests pass.\n\n**Files to modify:**\n- src/engine.py\n- src/cli.py\n- tests/test_process_compliance.py (tests already exist)\n\n**Success criteria:**\n- `orchestrator status --json` outputs valid JSON\n- JSON includes active, workflow_id, task, phase, progress, items, constraints\n- All TestStatusJson tests pass\n",
      "dependencies": [],
      "status": "done"
    }
  ],
  "current_task": null,
  "completed_tasks": ["wf-014-review-validation", "wf-012-context-reminder", "wf-013-verify-write", "wf-015-status-json"],
  "checkpoint_interval": 5
}
