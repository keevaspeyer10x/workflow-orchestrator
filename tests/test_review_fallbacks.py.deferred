"""
Tests for review fallback system.

These tests verify that:
1. When primary model fails, fallback is tried
2. When all fallbacks fail, returns None
3. Minimum review threshold is enforced
4. Insufficient reviews behavior (warn vs block)
5. Fallback usage is logged
"""

import pytest
from unittest.mock import Mock, patch, AsyncMock
from src.schema import ReviewSettings
from src.review.router import ReviewRouter


class TestReviewFallbackBasic:
    """Test basic fallback behavior."""

    @pytest.mark.asyncio
    async def test_fallback_on_primary_failure(self):
        """When primary model fails, should try fallback."""
        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1", "anthropic/claude-opus-4"]
            }
        )
        router = ReviewRouter(settings)

        # Mock: Primary fails, first fallback succeeds
        with patch.object(router, '_call_primary_model', side_effect=Exception("API Error")):
            with patch.object(router, '_call_fallback_model', return_value={"issues": [], "model_used": "openai/gpt-5.1"}):
                result = await router.run_review_with_fallback("codex", [], "diff")

        assert result is not None
        assert result["model_used"] == "openai/gpt-5.1"  # Fallback used

    @pytest.mark.asyncio
    async def test_primary_success_no_fallback(self):
        """When primary succeeds, fallback should not be tried."""
        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1"]
            }
        )
        router = ReviewRouter(settings)

        # Mock: Primary succeeds
        with patch.object(router, '_call_primary_model', return_value={"issues": [], "model_used": "openai/gpt-5.2-codex-max"}):
            result = await router.run_review_with_fallback("codex", [], "diff")

        assert result is not None
        assert result["model_used"] == "openai/gpt-5.2-codex-max"  # Primary used

    @pytest.mark.asyncio
    async def test_fallback_chain_exhausted(self):
        """When all fallbacks fail, should return None."""
        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1", "anthropic/claude-opus-4"]
            }
        )
        router = ReviewRouter(settings)

        # Mock: All models fail
        with patch.object(router, '_call_primary_model', side_effect=Exception("Fail")):
            with patch.object(router, '_call_fallback_model', side_effect=Exception("Fail")):
                result = await router.run_review_with_fallback("codex", [], "diff")

        assert result is None  # All fallbacks exhausted

    @pytest.mark.asyncio
    async def test_second_fallback_tried(self):
        """When first fallback fails, second should be tried."""
        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1", "anthropic/claude-opus-4"]
            }
        )
        router = ReviewRouter(settings)

        # Mock: Primary fails, first fallback fails, second succeeds
        with patch.object(router, '_call_primary_model', side_effect=Exception("Fail")):
            with patch.object(router, '_call_fallback_model') as mock_fallback:
                # First call fails, second succeeds
                mock_fallback.side_effect = [
                    Exception("First fallback failed"),
                    {"issues": [], "model_used": "anthropic/claude-opus-4"}
                ]

                result = await router.run_review_with_fallback("codex", [], "diff")

        assert result is not None
        assert result["model_used"] == "anthropic/claude-opus-4"  # Second fallback used


class TestMinimumReviewThreshold:
    """Test minimum review requirement enforcement."""

    def test_minimum_reviews_met(self):
        """When minimum reviews met, should return True."""
        settings = ReviewSettings(minimum_required=3)
        router = ReviewRouter(settings)

        completed_reviews = ["codex", "gemini", "grok"]  # 3 reviews
        result = router.check_minimum_reviews(completed_reviews)

        assert result is True  # Minimum met, continue

    def test_minimum_reviews_exceeded(self):
        """When reviews exceed minimum, should return True."""
        settings = ReviewSettings(minimum_required=3)
        router = ReviewRouter(settings)

        completed_reviews = ["codex", "gemini", "grok", "claude", "gpt4"]  # 5 reviews
        result = router.check_minimum_reviews(completed_reviews)

        assert result is True  # More than minimum

    def test_insufficient_reviews_warn_mode(self, caplog):
        """In warn mode, insufficient reviews should log warning."""
        import logging

        settings = ReviewSettings(
            minimum_required=3,
            on_insufficient_reviews="warn"
        )
        router = ReviewRouter(settings)

        completed_reviews = ["codex"]  # Only 1 review

        with caplog.at_level(logging.WARNING):
            result = router.check_minimum_reviews(completed_reviews)

        assert result is False  # Threshold not met
        assert "Only 1 of 3 required reviews completed" in caplog.text

    def test_insufficient_reviews_block_mode(self):
        """In block mode, insufficient reviews should raise exception."""
        from src.review.router import InsufficientReviewsError

        settings = ReviewSettings(
            minimum_required=3,
            on_insufficient_reviews="block"
        )
        router = ReviewRouter(settings)

        completed_reviews = ["codex"]  # Only 1 review

        with pytest.raises(InsufficientReviewsError) as exc_info:
            router.check_minimum_reviews(completed_reviews)

        assert "Only 1 of 3" in str(exc_info.value)

    def test_zero_reviews_block_mode(self):
        """Zero reviews in block mode should raise exception."""
        from src.review.router import InsufficientReviewsError

        settings = ReviewSettings(
            minimum_required=3,
            on_insufficient_reviews="block"
        )
        router = ReviewRouter(settings)

        completed_reviews = []  # No reviews

        with pytest.raises(InsufficientReviewsError):
            router.check_minimum_reviews(completed_reviews)


class TestReviewFallbackLogging:
    """Test logging of fallback usage."""

    @pytest.mark.asyncio
    async def test_fallback_usage_logged(self, caplog):
        """When fallback used, should log which model was used."""
        import logging

        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1"]
            }
        )
        router = ReviewRouter(settings)

        with patch.object(router, '_call_primary_model', side_effect=Exception("Fail")):
            with patch.object(router, '_call_fallback_model', return_value={"issues": [], "model_used": "openai/gpt-5.1"}):
                with caplog.at_level(logging.INFO):
                    await router.run_review_with_fallback("codex", [], "diff")

        # Should log that fallback was used
        assert any("fallback" in record.message.lower() for record in caplog.records)
        assert any("openai/gpt-5.1" in record.message for record in caplog.records)

    @pytest.mark.asyncio
    async def test_primary_failure_logged(self, caplog):
        """When primary fails, failure should be logged."""
        import logging

        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1"]
            }
        )
        router = ReviewRouter(settings)

        with patch.object(router, '_call_primary_model', side_effect=Exception("API Error")):
            with patch.object(router, '_call_fallback_model', return_value={"issues": []}):
                with caplog.at_level(logging.WARNING):
                    await router.run_review_with_fallback("codex", [], "diff")

        # Should log primary failure
        assert any("primary model" in record.message.lower() for record in caplog.records)
        assert any("failed" in record.message.lower() for record in caplog.records)

    @pytest.mark.asyncio
    async def test_all_fallbacks_exhausted_logged(self, caplog):
        """When all fallbacks fail, should log error."""
        import logging

        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1", "anthropic/claude-opus-4"]
            }
        )
        router = ReviewRouter(settings)

        with patch.object(router, '_call_primary_model', side_effect=Exception("Fail")):
            with patch.object(router, '_call_fallback_model', side_effect=Exception("Fail")):
                with caplog.at_level(logging.ERROR):
                    result = await router.run_review_with_fallback("codex", [], "diff")

        # Should log exhaustion error
        assert any("exhausted" in record.message.lower() for record in caplog.records)
        assert result is None


class TestReviewFallbackConfiguration:
    """Test review fallback configuration."""

    def test_fallback_configuration_loaded(self):
        """Fallback configuration should load correctly."""
        settings = ReviewSettings(
            fallbacks={
                "codex": ["openai/gpt-5.1", "anthropic/claude-opus-4"],
                "gemini": ["google/gemini-3-pro"],
                "grok": ["x-ai/grok-4.1"]
            }
        )

        assert len(settings.fallbacks["codex"]) == 2
        assert settings.fallbacks["codex"][0] == "openai/gpt-5.1"
        assert settings.fallbacks["gemini"][0] == "google/gemini-3-pro"
        assert settings.fallbacks["grok"][0] == "x-ai/grok-4.1"

    def test_default_fallbacks(self):
        """Default fallbacks should be sensible."""
        settings = ReviewSettings()

        # Should have fallbacks for main models
        assert "codex" in settings.fallbacks
        assert "gemini" in settings.fallbacks
        assert "grok" in settings.fallbacks

        # Fallbacks should include OpenRouter and Claude
        assert any("openai" in fb for fb in settings.fallbacks["codex"])
        assert any("anthropic" in fb for fb in settings.fallbacks["codex"])

    def test_empty_fallbacks_allowed(self):
        """Empty fallbacks should be allowed (no fallback chain)."""
        settings = ReviewSettings(
            fallbacks={
                "codex": []  # No fallbacks
            }
        )

        assert settings.fallbacks["codex"] == []


class TestOpenRouterIntegration:
    """Test OpenRouter provider integration."""

    @pytest.mark.asyncio
    async def test_openrouter_api_call(self):
        """OpenRouter API should be callable."""
        from src.providers.openrouter import OpenRouterProvider

        provider = OpenRouterProvider(api_key="test_key")

        # Mock the API response
        with patch('aiohttp.ClientSession.post') as mock_post:
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json = AsyncMock(return_value={
                "choices": [{"message": {"content": "Review result"}}]
            })
            mock_post.return_value.__aenter__.return_value = mock_response

            result = await provider.call_model(
                model="openai/gpt-5.1",
                prompt="Review this code"
            )

        assert result == "Review result"

    def test_openrouter_api_key_required(self):
        """OpenRouter should require API key."""
        from src.providers.openrouter import OpenRouterProvider

        with pytest.raises(ValueError, match="API key"):
            OpenRouterProvider(api_key=None)

    @pytest.mark.asyncio
    async def test_openrouter_error_handling(self):
        """OpenRouter should handle API errors gracefully."""
        from src.providers.openrouter import OpenRouterProvider

        provider = OpenRouterProvider(api_key="test_key")

        # Mock API error
        with patch('aiohttp.ClientSession.post') as mock_post:
            mock_response = AsyncMock()
            mock_response.status = 500
            mock_response.raise_for_status = Mock(side_effect=Exception("API Error"))
            mock_post.return_value.__aenter__.return_value = mock_response

            with pytest.raises(Exception, match="API Error"):
                await provider.call_model(
                    model="openai/gpt-5.1",
                    prompt="Review this code"
                )


class TestReviewIntegration:
    """Test review fallback integration with workflow."""

    @pytest.mark.asyncio
    async def test_multiple_model_reviews_with_fallbacks(self):
        """Multiple models should each use fallbacks if needed."""
        settings = ReviewSettings(
            minimum_required=2,
            fallbacks={
                "codex": ["openai/gpt-5.1"],
                "gemini": ["google/gemini-3-pro"],
                "grok": ["x-ai/grok-4.1"]
            }
        )
        router = ReviewRouter(settings)

        # Mock: codex primary fails (uses fallback), gemini succeeds, grok fails (uses fallback)
        completed = []

        # Codex: primary fails, fallback succeeds
        with patch.object(router, '_call_primary_model', side_effect=Exception("Fail")):
            with patch.object(router, '_call_fallback_model', return_value={"issues": [], "model_used": "openai/gpt-5.1"}):
                result = await router.run_review_with_fallback("codex", [], "diff")
                if result:
                    completed.append("codex")

        # Gemini: primary succeeds
        with patch.object(router, '_call_primary_model', return_value={"issues": [], "model_used": "gemini-3-pro-preview"}):
            result = await router.run_review_with_fallback("gemini", [], "diff")
            if result:
                completed.append("gemini")

        # Check minimum met
        assert len(completed) >= 2
        assert router.check_minimum_reviews(completed) is True

    def test_review_settings_in_workflow(self):
        """Review settings should integrate with workflow settings."""
        from src.schema import WorkflowSettings

        settings = WorkflowSettings(
            supervision_mode="zero_human",
            reviews={
                "enabled": True,
                "minimum_required": 3,
                "fallbacks": {
                    "codex": ["openai/gpt-5.1"]
                },
                "on_insufficient_reviews": "warn"
            }
        )

        assert settings.reviews.enabled is True
        assert settings.reviews.minimum_required == 3
        assert settings.reviews.on_insufficient_reviews == "warn"
